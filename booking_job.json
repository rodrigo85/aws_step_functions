{
	"jobConfig": {
		"name": "booking_job",
		"description": "",
		"role": "arn:aws:iam::yyyyyyyyyyyyyyyyyyy:role/glue_role",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 120,
		"maxConcurrentRuns": 10,
		"delay": 5,
		"security": "none",
		"scriptName": "bookings_job.py",
		"scriptLocation": "s3://glue-repo-code/bookings/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-09-04T14:48:38.936Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://glue-repo-code/bookings/tmp/",
		"etlAutoScaling": true,
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"pythonPath": "s3://glue-repo-code/libraries/etl_utils.py",
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://glue-repo-code/bookings/sparkHistoryLogs/",
		"flexExecution": true,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nimport json\r\nimport time\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.utils import getResolvedOptions\r\n\r\n# Initialize the Glue and Spark context\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\n\r\ndef process_job(job):\r\n    \"\"\"\r\n    Process each SQL job: read tables, execute SQL, and write results to S3.\r\n    \"\"\"\r\n    print(\"Processing job...\")\r\n    sql_text = job.get('sql_text', 'No SQL text found')\r\n    for table in job.get('tables'):\r\n        table_name = table[\"table_name\"]\r\n        s3_path = table[\"s3_path\"]\r\n        df = spark.read.parquet(s3_path)\r\n        df.printSchema()\r\n        df.createOrReplaceTempView(table_name)\r\n        print(f\"Loaded Temp view '{table_name}' from {s3_path}.\")\r\n    \r\n    result_df = spark.sql(sql_text)\r\n    s3_target = job.get('s3_target')\r\n    if s3_target:            \r\n        result_df.write.mode(\"overwrite\").parquet(s3_target)\r\n        \r\n    print(f\"Finished processing job: {job['sql_name']}\")\r\n\r\ndef main():\r\n    print(\"Starting job execution...\")\r\n    # Parse job arguments using getResolvedOptions\r\n    args = getResolvedOptions(sys.argv, ['group_list', 'JOB_NAME'])\r\n    \r\n    # Deserialize the JSON string into a Python list of jobs\r\n    group_jobs_json = args['group_list']\r\n    group_jobs = json.loads(group_jobs_json)\r\n    \r\n    # Initialize the Glue job\r\n    job = Job(glueContext)\r\n    job.init(args['JOB_NAME'], {}) \r\n    \r\n    # Process each job sequentially (no parallel processing)\r\n    for job_item in group_jobs:\r\n        process_job(job_item)\r\n    \r\n    print(\"All jobs processed.\")\r\n    \r\n    # Commit the Glue job\r\n    job.commit()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
}